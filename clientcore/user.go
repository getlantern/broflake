// user.go provides a facility for proxying user-generated bytestreams - ie, streams which
// originate from a process running on the user's computer (like a web browser) or streams which
// are generated as part of the user's Lantern control plane activity. We can operationalize a
// user stream in a WorkerFSM just like we do an RTCPeerConnection or a websocket.Conn, such that
// bytestreams from the user can be neatly managed alongside bytestreams from remote peers in
// their consumer table. In other words: we treat the user's web browser just like any connected consumer.
package clientcore

import (
	"context"
	"fmt"
	"io"
	"net"
	"sync"
	"time"

	"github.com/xtaci/smux"
)

type DialerFn func(network string, addr string) (net.Conn, error)

type UserStreamSource interface {
	InitWithDialer(dial DialerFn)
}

func NewProducerUserStream(source UserStreamSource, wg *sync.WaitGroup) *WorkerFSM {
	return NewWorkerFSM([]FSMstate{
		FSMstate(func(ctx context.Context, com *ipcChan, input []interface{}) (int, []interface{}) {
			// State 0
			// (no input data)
			fmt.Printf("User stream producer state 0...\n")

			// TODO: check for a non-nil path assertion to alert the UI that we're ready to proxy?

			// We're downstream, so we read from the downstream channel. This pipe is used by our
			// userConn wrapper to buffer inbound data. We write to the pipe when data arrives from the
			// bus, and the underlying TCPConn reads from the pipe when the controlling process (e.g.,
			// the user's browser) executes a read on the socket.
			downRead, downWrite := io.Pipe()

			// Our model for multiplexing: this WorkerFSM is associated with a single smux session. Each
			// time this WorkerFSM dials, that's a stream within the session.
			workerConn := userConn{Conn: &net.TCPConn{}, readPipe: downRead, writeChan: com.tx}

			// The default value for MaxFrameSize (32K) makes the WebSocket library complain about oversized msgs
			// TODO: it's not clear that 16K is optimal, we should run some experiments - and we should fetch
			// these configuration values from the 'common' module to ensure agreement across client and server
			smuxCfg := smux.DefaultConfig()
			smuxCfg.KeepAliveDisabled = true
			smuxCfg.MaxFrameSize = 16384
			err := smux.VerifyConfig(smuxCfg)
			if err != nil {
				panic(err)
			}

			workerSess, err := smux.Client(&workerConn, smuxCfg)
			if err != nil {
				panic(err)
			}

			dialer := func(network string, addr string) (net.Conn, error) {
				clientStream, err := workerSess.OpenStream()
				if err != nil {
					panic(err)
				}
				return clientStream, nil
			}

			source.InitWithDialer(dialer)

			return 1, []interface{}{downWrite}
		}),
		FSMstate(func(ctx context.Context, com *ipcChan, input []interface{}) (int, []interface{}) {
			// State 1
			// input[0]: *io.PipeWriter
			downWrite := input[0].(*io.PipeWriter)
			fmt.Printf("User stream producer state 1...\n")

			// TODO: it's spaghetti-ish that we unwrap inbound chunks from the bus here, but we wrap
			// outbound chunks in userConn.Write below. If we buffered reads using a real data
			// structure (like a ring buffer) instead of exploiting io.Pipe, this would be easy to fix.
			// But then we'd have more code to maintain...
			for {
				msg := <-com.rx
				if msg.IpcType == ChunkIPC {
					downWrite.Write(msg.Data.([]byte))
				}
			}

			// TODO: reminder that there's no way to error out and exit state 1, per comments above...
			// this code path should be unreachable.
			return 0, []interface{}{}
		}),
	}, wg)
}

type userConn struct {
	net.Conn
	readPipe      *io.PipeReader
	writeChan     chan IpcMsg
	readDeadline  time.Time
	writeDeadline time.Time
	cancelRead    func()
	cancelWrite   func()
	readCtx       context.Context
	writeCtx      context.Context
	sync.RWMutex
}

// TODO: we ought to monitor whether our connections are being closed and tidied up
func (c *userConn) Close() error {
	return c.Conn.Close()
}

func (c *userConn) Read(b []byte) (n int, err error) {
	c.RLock()
	defer c.RUnlock()

	// 3 cases:
	// A) We have a deadline that's already expired, so don't execute this read
	// B) We have a future deadline, so execute this read with it
	// C) We have a zero value deadline, so execute this read with no timeout
	if time.Time.IsZero(c.readDeadline) {
		c.readCtx, c.cancelRead = context.WithCancel(context.Background())
	} else if c.readDeadline.Before(time.Now()) {
		return 0, nil
	} else {
		c.readCtx, c.cancelRead = context.WithDeadline(context.Background(), c.readDeadline)
	}

	defer c.cancelRead()
	doneChan := make(chan struct {
		n   int
		err error
	}, 1)

	func() {
		n, err = c.readPipe.Read(b)
		doneChan <- struct {
			n   int
			err error
		}{n: n, err: err}
	}()

	select {
	case res := <-doneChan:
		return res.n, res.err
	case <-c.readCtx.Done():
		return 0, c.readCtx.Err()
	}
}

func (c *userConn) SetReadDeadline(t time.Time) error {
	c.Lock()
	defer c.Unlock()

	if t.Equal(c.readDeadline) {
		return nil
	}

	if t.Before(c.readDeadline) || time.Time.IsZero(c.readDeadline) {
		// TODO: this nil check can likely be avoided by initializing userConn.cancelRead with a func()
		if c.cancelRead != nil {
			c.cancelRead()
		}
	}

	c.readDeadline = t
	return nil
}

func (c *userConn) Write(b []byte) (n int, err error) {
	c.RLock()
	defer c.RUnlock()

	// Prevents a data race on the underlying net.TCPConn buffer
	bb := make([]byte, len(b))
	copy(bb, b)

	// 3 cases:
	// A) We have a deadline that's already expired, so don't execute this read
	// B) We have a future deadline, so execute this read with it
	// C) We have a zero value deadline, so execute this read with no timeout
	if time.Time.IsZero(c.writeDeadline) {
		c.writeCtx, c.cancelWrite = context.WithCancel(context.Background())
	} else if c.writeDeadline.Before(time.Now()) {
		return 0, nil
	} else {
		c.writeCtx, c.cancelWrite = context.WithDeadline(context.Background(), c.writeDeadline)
	}

	defer c.cancelWrite()

	select {
	case c.writeChan <- IpcMsg{IpcType: ChunkIPC, Data: bb}:
		return len(bb), err
	case <-c.writeCtx.Done():
		panic("Producer user stream buffer overflow!")
		// TODO: We almost definitely shouldn't panic here, and this case could actually either be a
		// channel buffer overflow OR a context deadline exceeded. Must disentangle this.
		return 0, c.writeCtx.Err()
	}
}

func (c *userConn) SetWriteDeadline(t time.Time) error {
	c.Lock()
	defer c.Unlock()

	if t.Equal(c.writeDeadline) {
		return nil
	}

	if t.Before(c.writeDeadline) || time.Time.IsZero(c.writeDeadline) {
		// TODO: this nil check can likely be avoided by initializing userConn.cancelWrite with a func()
		if c.cancelWrite != nil {
			c.cancelWrite()
		}
	}

	c.writeDeadline = t
	return nil
}

func (c *userConn) SetDeadline(t time.Time) error {
	c.SetReadDeadline(t)
	c.SetWriteDeadline(t)
	return nil
}
