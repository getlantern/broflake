// user.go provides a facility for proxying user-generated bytestreams - ie, streams which 
// originate from a process running on the user's computer (like a web browser) or streams which 
// are generated as part of the user's Lantern control plane activity. We can operationalize a
// user stream in a workerFSM just like we do an RTCPeerConnection or a websocket.Conn, such that
// bytestreams from the user can be neatly managed alongside bytestreams from remote peers in
// their consumer table. In other words: we treat the user's web browser just like any connected consumer.
package main

// TODO: this file is where we hook Flashlight and wrangle the streams in and out of the Lantern
// desktop mothership. To avoid the complexity of hooking Flashlight during prototype development, 
// we mock Flashlight-like functionality by incorporating our own HTTP CONNECT proxy. Our assumption: 
// if we can correctly proxy bytestreams originating from our local HTTP proxy now, we'll be able 
// to proxy bytestreams originating from Flashlight later. 

import (
  "context"
  "fmt"
  "io"
  "net"
  "net/http"
  "net/url"
  "sync"
  "time"
  
  "github.com/elazarl/goproxy"
  "github.com/xtaci/smux"
)

func newProducerUserStream(proxyAddr string) *workerFSM {
  return newWorkerFSM([]FSMstate{
    FSMstate(func(com *ipcChan, input []interface{}) (int, []interface{}) {
      // State 0
      // (no input data)
      fmt.Printf("User stream producer state 0...\n")
      
      // TODO: check for a non-nil path assertion to alert the UI that we're ready to proxy?
      
      // We're downstream, so we read from the downstream channel. This pipe is used by our 
      // userConn wrapper to buffer inbound data. We write to the pipe when data arrives from the
      // bus, and the underlying TCPConn reads from the pipe when the controlling process (e.g., 
      // the user's browser) executes a read on the socket. 
      downRead, downWrite := io.Pipe()
      
      // Our model for multiplexing: this workerFSM is associated with a single smux session. Each 
      // time this workerFSM dials, that's a stream within the session.
      workerConn := userConn{Conn: &net.TCPConn{}, readPipe: downRead, writeChan: com.tx}
      
      // The default value for MaxFrameSize (32K) makes the WebSocket library complain about oversized msgs
      // TODO: it's not clear that 16K is optimal, we should run some experiments - and we should fetch
      // these configuration values from the 'common' module to ensure agreement across client and server
      smuxCfg := smux.DefaultConfig()
      smuxCfg.KeepAliveDisabled = true
      smuxCfg.MaxFrameSize = 16384
      err := smux.VerifyConfig(smuxCfg)
      if err != nil {
        panic(err)
      }

      workerSess, err := smux.Client(&workerConn, smuxCfg)
      if err != nil {
        panic(err)
      }

      // Instantiate our local HTTP CONNECT proxy
      proxy := goproxy.NewProxyHttpServer()
      proxy.Verbose = true
      // This tells goproxy to wrap the dial function in a chained CONNECT request
      proxy.ConnectDial = proxy.NewConnectDialToProxy("http://i.do.nothing")
      
      proxy.Tr = &http.Transport{
        Dial: func(network string, addr string) (net.Conn, error) {
          clientStream, err := workerSess.OpenStream()
          if err != nil {
            panic(err)
          }
          return clientStream, nil
        },
        // goproxy requires this to make things work
        Proxy: func(req *http.Request) (*url.URL, error) {
          return url.Parse("http://i.do.nothing")
        },
      }

      fmt.Printf("Starting HTTP CONNECT proxy on %v...\n", proxyAddr)

      go func() {
        err := http.ListenAndServe(proxyAddr, proxy)
        if err != nil {
          panic(err)
        }
        // TODO: if this wasn't just mocked functionality, we'd probably want a channel to 
        // propagate forward into state 1 over which we could listen for the error returned here...
      }()

      return 1, []interface{}{proxy, downWrite}
    }),
    FSMstate(func(com *ipcChan, input []interface{}) (int, []interface{})  {
      // State 1
      // input[0]: *goproxy.ProxyHttpServer
      // input[1]: *io.PipeWriter
      proxy := input[0].(*goproxy.ProxyHttpServer)
      downWrite := input[1].(*io.PipeWriter)
      fmt.Printf("User stream producer state 1...\n")
      
      proxy.OnRequest().DoFunc(
        func(r *http.Request, ctx *goproxy.ProxyCtx) (*http.Request, *http.Response) {
          fmt.Println("HTTP proxy just saw a request:")
          fmt.Println(r)
          return r, nil
        },
      )
      
      // TODO: it's spaghetti-ish that we unwrap inbound chunks from the bus here, but we wrap
      // outbound chunks in userConn.Write below. If we buffered reads using a real data 
      // structure (like a ring buffer) instead of exploiting io.Pipe, this would be easy to fix.
      // But then we'd have more code to maintain...
      for {
        msg := <-com.rx
        if msg.ipcType == ChunkIPC {
          downWrite.Write(msg.data.([]byte))
        }
      }
      
      // TODO: reminder that there's no way to error out and exit state 1, per comments above...
      // this code path should be unreachable.
      return 0, []interface{}{}
    }),
  })
}

type userConn struct{
  net.Conn
  readPipe *io.PipeReader
  writeChan chan ipcMsg
  readDeadline time.Time
  writeDeadline time.Time
  cancelRead func()
  cancelWrite func()
  readCtx context.Context
  writeCtx context.Context
  sync.RWMutex
}

// TODO: we ought to monitor whether our connections are being closed and tidied up
func (c *userConn) Close() error {
  return c.Conn.Close()
}

func (c *userConn) Read(b []byte) (n int, err error) {
  c.RLock()
  defer c.RUnlock()

  // 3 cases: 
  // A) We have a deadline that's already expired, so don't execute this read
  // B) We have a future deadline, so execute this read with it
  // C) We have a zero value deadline, so execute this read with no timeout
  if time.Time.IsZero(c.readDeadline) {
    c.readCtx, c.cancelRead = context.WithCancel(context.Background())
  } else if c.readDeadline.Before(time.Now()) {
    return 0, nil
  } else {
    c.readCtx, c.cancelRead = context.WithDeadline(context.Background(), c.readDeadline)
  }

  defer c.cancelRead()
  doneChan := make(chan struct{n int; err error}, 1)
  
  func () {
    n, err = c.readPipe.Read(b)
    doneChan <-struct{n int; err error}{n: n, err: err}
  }()
  
  select {
  case res := <-doneChan:
    return res.n, res.err
  case <-c.readCtx.Done():
    return 0, c.readCtx.Err()
  }
}

func (c *userConn) SetReadDeadline(t time.Time) error {
  c.Lock()
  defer c.Unlock()

  if t.Equal(c.readDeadline) {
    return nil
  }

  if t.Before(c.readDeadline) || time.Time.IsZero(c.readDeadline) {
    // TODO: this nil check can likely be avoided by initializing userConn.cancelRead with a func()
    if c.cancelRead != nil {
      c.cancelRead()
    }
  }

  c.readDeadline = t
  return nil
}

func (c *userConn) Write(b []byte) (n int, err error) {
  c.RLock()
  defer c.RUnlock()
  
  // Prevents a data race on the underlying net.TCPConn buffer
  bb := make([]byte, len(b))
  copy(bb, b)

  // 3 cases: 
  // A) We have a deadline that's already expired, so don't execute this read
  // B) We have a future deadline, so execute this read with it
  // C) We have a zero value deadline, so execute this read with no timeout
  if time.Time.IsZero(c.writeDeadline) {
    c.writeCtx, c.cancelWrite = context.WithCancel(context.Background())
  } else if c.writeDeadline.Before(time.Now()) {
    return 0, nil
  } else {
    c.writeCtx, c.cancelWrite = context.WithDeadline(context.Background(), c.writeDeadline)
  }

  defer c.cancelWrite()
  
  select {
  case c.writeChan <-ipcMsg{ipcType: ChunkIPC, data: bb}:
    return len(bb), err
  case <-c.writeCtx.Done():
    panic("Producer user stream buffer overflow!")
    // TODO: We almost definitely shouldn't panic here, and this case could actually either be a
    // channel buffer overflow OR a context deadline exceeded. Must disentangle this.
    return 0, c.writeCtx.Err()
  }
}

func (c *userConn) SetWriteDeadline(t time.Time) error {
  c.Lock()
  defer c.Unlock()

  if t.Equal(c.writeDeadline) {
    return nil
  }

  if t.Before(c.writeDeadline) || time.Time.IsZero(c.writeDeadline) {
    // TODO: this nil check can likely be avoided by initializing userConn.cancelWrite with a func()
    if c.cancelWrite != nil {
      c.cancelWrite()
    }
  }

  c.writeDeadline = t
  return nil
}

func (c *userConn) SetDeadline(t time.Time) error {
  c.SetReadDeadline(t)
  c.SetWriteDeadline(t)
  return nil
}
